# -*- coding: utf-8 -*-
"""PDF Comparison Script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I2-oQMqXtsLkHe3nN-tKmA-DI2idG-TF
"""

!pip install spacy
!python -m spacy download en_core_web_md

import spacy
import PyPDF2
from collections import Counter

# Load the spaCy model and increase max_length
nlp = spacy.load("en_core_web_md")
nlp.max_length = 2000000  # Adjust as needed

def extract_text_from_pdf(pdf_path):
    """Extract text content from a PDF file."""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
    return text

def compute_similarity(text1, text2):
    """Compute similarity between two texts using spaCy."""
    doc1 = nlp(text1)
    doc2 = nlp(text2)
    return doc1.similarity(doc2) * 100

def generate_ngrams(tokens, n=2):
    """Generate n-grams from tokens."""
    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

def analyze_ngrams(text, n=2):
    """Analyze n-grams in a text."""
    tokens = [token.text for token in nlp(text) if not token.is_stop]
    ngrams = generate_ngrams(tokens, n)
    return Counter(ngrams)

def ngram_similarity(counter1, counter2):
    """Compute the similarity between two n-gram Counters."""
    common_ngrams = sum((counter1 & counter2).values())
    total_ngrams = sum((counter1 | counter2).values())
    return (common_ngrams / total_ngrams) * 100

def sentence_similarity(text1, text2):
    """Compute sentence-level similarity using spaCy."""
    doc1_sentences = list(nlp(text1).sents)
    doc2_sentences = list(nlp(text2).sents)
    similarities = [sent1.similarity(sent2) for sent1 in doc1_sentences for sent2 in doc2_sentences]
    return sum(similarities) / len(similarities) * 100

def main(pdf1_path, pdf2_path):
    text1 = extract_text_from_pdf(pdf1_path)
    text2 = extract_text_from_pdf(pdf2_path)

    overall_similarity = compute_similarity(text1, text2)

    bigrams_doc1 = analyze_ngrams(text1, 2)
    bigrams_doc2 = analyze_ngrams(text2, 2)
    bigram_similarity = ngram_similarity(bigrams_doc1, bigrams_doc2)

    trigrams_doc1 = analyze_ngrams(text1, 3)
    trigrams_doc2 = analyze_ngrams(text2, 3)
    trigram_similarity = ngram_similarity(trigrams_doc1, trigrams_doc2)

    sent_similarity = sentence_similarity(text1, text2)

    print(f"\nDocument Similarity: {overall_similarity:.2f}%")
    print(f"\nBigram Similarity: {bigram_similarity:.2f}%")
    print(f"\nTrigram Similarity: {trigram_similarity:.2f}%")
    print(f"\nSentence-level similarity: {sent_similarity:.2f}%")

if __name__ == "__main__":
    pdf1_path = input("Enter path to first PDF: ")
    pdf2_path = input("Enter path to second PDF: ")
    main(pdf1_path, pdf2_path)